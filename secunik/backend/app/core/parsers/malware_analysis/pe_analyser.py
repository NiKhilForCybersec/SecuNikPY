"""
SecuNik - PE Analyzer (Portable Executable Analysis) - Pure Data Extractor
Extracts raw PE file data for AI analysis

Location: backend/app/core/parsers/malware_analysis/pe_analyzer.py
"""

import json
import logging
import hashlib
import struct
from datetime import datetime
from typing import Dict, List, Optional, Any, Set
from pathlib import Path
import math

try:
    import pefile
    import yara
    PEFILE_AVAILABLE = True
    YARA_AVAILABLE = True
except ImportError:
    PEFILE_AVAILABLE = False
    YARA_AVAILABLE = False

from ...models.analysis import AnalysisResult, Severity, IOC, IOCType

logger = logging.getLogger(__name__)

class PEAnalyzer:
    """Pure Data Extractor for PE Files - AI analyzes the data"""
    
    def __init__(self):
        self.name = "PE File Analyzer"
        self.version = "2.0.0"
        self.supported_formats = [".exe", ".dll", ".sys", ".scr", ".com"]

    def can_parse(self, file_path: str) -> bool:
        """Check if file can be parsed by this parser"""
        return Path(file_path).suffix.lower() in self.supported_formats and PEFILE_AVAILABLE

    def parse(self, file_path: str) -> AnalysisResult:
        """Extract raw PE file data - AI will analyze for threats"""
        try:
            if not PEFILE_AVAILABLE:
                return self._create_error_result("pefile library not available")
            
            logger.info(f"Extracting PE file data: {file_path}")
            
            # Load PE file
            pe = pefile.PE(file_path)
            
            # Extract comprehensive PE data
            pe_data = self._extract_pe_data(pe, file_path)
            
            # Extract factual IOCs
            iocs = self._extract_factual_iocs(pe_data, file_path)
            
            # Close PE file
            pe.close()
            
            # Create analysis result with extracted data
            result = AnalysisResult(
                file_path=file_path,
                parser_name=self.name,
                analysis_type="PE File Data Extraction",
                timestamp=datetime.now(),
                summary=f"Extracted PE file data for AI analysis",
                details=pe_data,
                threats_detected=[],  # AI will determine threats
                iocs_found=iocs,
                severity=Severity.LOW,  # AI will determine severity
                risk_score=0.0,  # AI will calculate risk score
                recommendations=["Data extracted - pending AI analysis for threat assessment and recommendations"]
            )
            
            logger.info("PE data extraction completed")
            return result
            
        except Exception as e:
            logger.error(f"Error extracting PE data {file_path}: {str(e)}")
            return self._create_error_result(str(e))

    def _extract_pe_data(self, pe: pefile.PE, file_path: str) -> Dict[str, Any]:
        """Extract comprehensive PE file data"""
        pe_data = {
            "file_info": self._get_file_info(file_path),
            "pe_header": self._extract_pe_header(pe),
            "sections": self._extract_sections(pe),
            "imports": self._extract_imports(pe),
            "exports": self._extract_exports(pe),
            "resources": self._extract_resources(pe),
            "entropy_analysis": self._analyze_entropy(pe),
            "strings_analysis": self._analyze_strings(file_path),
            "version_info": self._extract_version_info(pe),
            "digital_signature": self._extract_digital_signature(pe),
            "packer_indicators": self._extract_packer_indicators(pe),
            "architecture_info": self._extract_architecture_info(pe),
            "compilation_info": self._extract_compilation_info(pe)
        }
        
        # Perform YARA scanning if available
        if YARA_AVAILABLE:
            pe_data["yara_matches"] = self._yara_scan(file_path)
        
        return pe_data

    def _get_file_info(self, file_path: str) -> Dict[str, Any]:
        """Get basic file information"""
        file_stat = Path(file_path).stat()
        
        # Calculate file hashes
        with open(file_path, 'rb') as f:
            data = f.read()
            
        file_info = {
            "file_path": file_path,
            "file_name": Path(file_path).name,
            "file_size": file_stat.st_size,
            "md5": hashlib.md5(data).hexdigest(),
            "sha1": hashlib.sha1(data).hexdigest(),
            "sha256": hashlib.sha256(data).hexdigest(),
            "creation_time": datetime.fromtimestamp(file_stat.st_ctime).isoformat(),
            "modification_time": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
            "access_time": datetime.fromtimestamp(file_stat.st_atime).isoformat()
        }
        
        return file_info

    def _extract_pe_header(self, pe: pefile.PE) -> Dict[str, Any]:
        """Extract PE header information"""
        pe_header = {
            "machine_type": hex(pe.FILE_HEADER.Machine),
            "machine_type_name": self._get_machine_type_name(pe.FILE_HEADER.Machine),
            "number_of_sections": pe.FILE_HEADER.NumberOfSections,
            "time_date_stamp": pe.FILE_HEADER.TimeDateStamp,
            "compile_time": datetime.fromtimestamp(pe.FILE_HEADER.TimeDateStamp).isoformat(),
            "characteristics": hex(pe.FILE_HEADER.Characteristics),
            "characteristics_list": self._parse_characteristics(pe.FILE_HEADER.Characteristics),
            "optional_header": {}
        }
        
        # Optional header information
        if hasattr(pe, 'OPTIONAL_HEADER'):
            oh = pe.OPTIONAL_HEADER
            pe_header["optional_header"] = {
                "magic": hex(oh.Magic),
                "magic_type": "PE32" if oh.Magic == 0x10b else "PE32+" if oh.Magic == 0x20b else "Unknown",
                "entry_point": hex(oh.AddressOfEntryPoint),
                "image_base": hex(oh.ImageBase),
                "section_alignment": oh.SectionAlignment,
                "file_alignment": oh.FileAlignment,
                "size_of_image": oh.SizeOfImage,
                "size_of_headers": oh.SizeOfHeaders,
                "checksum": hex(oh.CheckSum),
                "subsystem": oh.Subsystem,
                "subsystem_name": self._get_subsystem_name(oh.Subsystem),
                "dll_characteristics": hex(oh.DllCharacteristics),
                "dll_characteristics_list": self._parse_dll_characteristics(oh.DllCharacteristics)
            }
        
        return pe_header

    def _get_machine_type_name(self, machine_type: int) -> str:
        """Get human-readable machine type name"""
        machine_types = {
            0x14c: "IMAGE_FILE_MACHINE_I386",
            0x8664: "IMAGE_FILE_MACHINE_AMD64",
            0x1c0: "IMAGE_FILE_MACHINE_ARM",
            0xaa64: "IMAGE_FILE_MACHINE_ARM64"
        }
        return machine_types.get(machine_type, f"Unknown (0x{machine_type:x})")

    def _get_subsystem_name(self, subsystem: int) -> str:
        """Get human-readable subsystem name"""
        subsystems = {
            1: "NATIVE", 2: "WINDOWS_GUI", 3: "WINDOWS_CUI", 5: "OS2_CUI",
            7: "POSIX_CUI", 8: "NATIVE_WINDOWS", 9: "WINDOWS_CE_GUI",
            10: "EFI_APPLICATION", 11: "EFI_BOOT_SERVICE_DRIVER",
            12: "EFI_RUNTIME_DRIVER", 13: "EFI_ROM", 14: "XBOX", 16: "WINDOWS_BOOT_APPLICATION"
        }
        return subsystems.get(subsystem, f"Unknown ({subsystem})")

    def _parse_characteristics(self, characteristics: int) -> List[str]:
        """Parse file characteristics flags"""
        flags = []
        characteristic_flags = {
            0x0001: "RELOCS_STRIPPED", 0x0002: "EXECUTABLE_IMAGE", 0x0004: "LINE_NUMBERS_STRIPPED",
            0x0008: "LOCAL_SYMS_STRIPPED", 0x0010: "AGGR_WS_TRIM", 0x0020: "LARGE_ADDRESS_AWARE",
            0x0080: "BYTES_REVERSED_LO", 0x0100: "32BIT_MACHINE", 0x0200: "DEBUG_STRIPPED",
            0x0400: "REMOVABLE_RUN_FROM_SWAP", 0x0800: "NET_RUN_FROM_SWAP", 0x1000: "SYSTEM",
            0x2000: "DLL", 0x4000: "UP_SYSTEM_ONLY", 0x8000: "BYTES_REVERSED_HI"
        }
        
        for flag, name in characteristic_flags.items():
            if characteristics & flag:
                flags.append(name)
        
        return flags

    def _parse_dll_characteristics(self, dll_characteristics: int) -> List[str]:
        """Parse DLL characteristics flags"""
        flags = []
        dll_flags = {
            0x0001: "PROCESS_INIT", 0x0002: "PROCESS_TERM", 0x0004: "THREAD_INIT",
            0x0008: "THREAD_TERM", 0x0040: "DYNAMIC_BASE", 0x0080: "FORCE_INTEGRITY",
            0x0100: "NX_COMPAT", 0x0200: "NO_ISOLATION", 0x0400: "NO_SEH",
            0x0800: "NO_BIND", 0x1000: "APPCONTAINER", 0x2000: "WDM_DRIVER",
            0x4000: "GUARD_CF", 0x8000: "TERMINAL_SERVER_AWARE"
        }
        
        for flag, name in dll_flags.items():
            if dll_characteristics & flag:
                flags.append(name)
        
        return flags

    def _extract_sections(self, pe: pefile.PE) -> List[Dict[str, Any]]:
        """Extract PE sections information"""
        sections = []
        
        for section in pe.sections:
            section_info = {
                "name": section.Name.decode('utf-8', errors='ignore').strip('\x00'),
                "virtual_address": hex(section.VirtualAddress),
                "virtual_size": section.Misc_VirtualSize,
                "raw_size": section.SizeOfRawData,
                "raw_address": hex(section.PointerToRawData),
                "characteristics": hex(section.Characteristics),
                "characteristics_list": self._parse_section_characteristics(section.Characteristics),
                "entropy": section.get_entropy(),
                "size_ratio": abs(section.Misc_VirtualSize - section.SizeOfRawData) / max(section.Misc_VirtualSize, section.SizeOfRawData, 1)
            }
            
            sections.append(section_info)
        
        return sections

    def _parse_section_characteristics(self, characteristics: int) -> List[str]:
        """Parse section characteristics flags"""
        flags = []
        section_flags = {
            0x00000020: "CODE", 0x00000040: "INITIALIZED_DATA", 0x00000080: "UNINITIALIZED_DATA",
            0x00000200: "LINK_INFO", 0x00000800: "LINK_REMOVE", 0x00001000: "LINK_COMDAT",
            0x00008000: "GPREL", 0x00020000: "MEM_PURGEABLE", 0x00020000: "MEM_16BIT",
            0x00040000: "MEM_LOCKED", 0x00080000: "MEM_PRELOAD", 0x20000000: "MEM_EXECUTE",
            0x40000000: "MEM_READ", 0x80000000: "MEM_WRITE"
        }
        
        for flag, name in section_flags.items():
            if characteristics & flag:
                flags.append(name)
        
        return flags

    def _extract_imports(self, pe: pefile.PE) -> Dict[str, Any]:
        """Extract imported functions information"""
        imports = {
            "total_imports": 0,
            "imported_dlls": [],
            "import_categories": {
                "system_apis": [],
                "network_apis": [],
                "file_apis": [],
                "registry_apis": [],
                "crypto_apis": [],
                "debug_apis": []
            }
        }
        
        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                dll_name = entry.dll.decode('utf-8', errors='ignore')
                dll_imports = []
                
                for imp in entry.imports:
                    if imp.name:
                        function_name = imp.name.decode('utf-8', errors='ignore')
                        dll_imports.append({
                            "name": function_name,
                            "ordinal": imp.ordinal,
                            "address": hex(imp.address) if imp.address else None
                        })
                        imports["total_imports"] += 1
                        
                        # Categorize API calls
                        self._categorize_api_call(function_name, imports["import_categories"])
                
                imports["imported_dlls"].append({
                    "dll_name": dll_name,
                    "functions": dll_imports,
                    "function_count": len(dll_imports)
                })
        
        return imports

    def _categorize_api_call(self, function_name: str, categories: Dict[str, List[str]]):
        """Categorize API call by functionality"""
        function_lower = function_name.lower()
        
        # System APIs
        if any(api in function_lower for api in ["virtualalloc", "virtualprotect", "createprocess", "openprocess"]):
            categories["system_apis"].append(function_name)
        # Network APIs  
        elif any(api in function_lower for api in ["wsastartup", "socket", "connect", "internetopen", "httpopen"]):
            categories["network_apis"].append(function_name)
        # File APIs
        elif any(api in function_lower for api in ["createfile", "readfile", "writefile", "copyfile", "movefile"]):
            categories["file_apis"].append(function_name)
        # Registry APIs
        elif any(api in function_lower for api in ["regopen", "regcreate", "regset", "regquery"]):
            categories["registry_apis"].append(function_name)
        # Crypto APIs
        elif any(api in function_lower for api in ["crypt", "hash", "encrypt", "decrypt"]):
            categories["crypto_apis"].append(function_name)
        # Debug APIs
        elif any(api in function_lower for api in ["isdebugger", "checkremotedebugger", "outputdebug"]):
            categories["debug_apis"].append(function_name)

    def _extract_exports(self, pe: pefile.PE) -> Dict[str, Any]:
        """Extract exported functions information"""
        exports = {
            "total_exports": 0,
            "exported_functions": [],
            "dll_name": ""
        }
        
        if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
            export_entry = pe.DIRECTORY_ENTRY_EXPORT
            if export_entry.name:
                exports["dll_name"] = export_entry.name.decode('utf-8', errors='ignore')
            
            for exp in export_entry.symbols:
                if exp.name:
                    function_name = exp.name.decode('utf-8', errors='ignore')
                    exports["exported_functions"].append({
                        "name": function_name,
                        "ordinal": exp.ordinal,
                        "address": hex(exp.address),
                        "rva": hex(exp.address - pe.OPTIONAL_HEADER.ImageBase) if hasattr(pe, 'OPTIONAL_HEADER') else None
                    })
                    exports["total_exports"] += 1
        
        return exports

    def _extract_resources(self, pe: pefile.PE) -> Dict[str, Any]:
        """Extract PE resources information"""
        resources = {
            "total_resources": 0,
            "resource_types": [],
            "resource_summary": {}
        }
        
        if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                try:
                    if resource_type.name is not None:
                        type_name = str(resource_type.name)
                    else:
                        type_name = pefile.RESOURCE_TYPE.get(resource_type.struct.Id, f"Unknown_{resource_type.struct.Id}")
                    
                    resource_info = {
                        "type": type_name,
                        "entries": []
                    }
                    
                    if hasattr(resource_type, 'directory'):
                        for resource_id in resource_type.directory.entries:
                            if hasattr(resource_id, 'directory'):
                                for resource_lang in resource_id.directory.entries:
                                    try:
                                        data = pe.get_data(resource_lang.data.struct.OffsetToData, 
                                                         resource_lang.data.struct.Size)
                                        
                                        entry_info = {
                                            "id": resource_id.struct.Id,
                                            "lang": resource_lang.struct.Id,
                                            "size": resource_lang.data.struct.Size,
                                            "entropy": self._calculate_entropy(data),
                                            "offset": hex(resource_lang.data.struct.OffsetToData)
                                        }
                                        
                                        resource_info["entries"].append(entry_info)
                                        resources["total_resources"] += 1
                                        
                                    except Exception as e:
                                        logger.warning(f"Error extracting resource: {e}")
                                        continue
                    
                    resources["resource_types"].append(resource_info)
                    resources["resource_summary"][type_name] = len(resource_info["entries"])
                    
                except Exception as e:
                    logger.warning(f"Error extracting resource type: {e}")
                    continue
        
        return resources

    def _analyze_entropy(self, pe: pefile.PE) -> Dict[str, Any]:
        """Analyze file entropy"""
        entropy_analysis = {
            "overall_entropy": 0,
            "section_entropies": [],
            "entropy_distribution": {}
        }
        
        # Calculate overall file entropy
        try:
            with open(pe.name, 'rb') as f:
                data = f.read()
                entropy_analysis["overall_entropy"] = self._calculate_entropy(data)
        except Exception as e:
            logger.warning(f"Error calculating overall entropy: {e}")
        
        # Analyze section entropies
        entropy_ranges = {"Low (0-3)": 0, "Medium (3-6)": 0, "High (6-8)": 0}
        
        for section in pe.sections:
            section_name = section.Name.decode('utf-8', errors='ignore').strip('\x00')
            section_entropy = section.get_entropy()
            
            entropy_info = {
                "section_name": section_name,
                "entropy": section_entropy
            }
            
            entropy_analysis["section_entropies"].append(entropy_info)
            
            # Categorize entropy
            if section_entropy <= 3:
                entropy_ranges["Low (0-3)"] += 1
            elif section_entropy <= 6:
                entropy_ranges["Medium (3-6)"] += 1
            else:
                entropy_ranges["High (6-8)"] += 1
        
        entropy_analysis["entropy_distribution"] = entropy_ranges
        
        return entropy_analysis

    def _calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of data"""
        if not data:
            return 0
        
        # Count byte frequencies
        byte_counts = [0] * 256
        for byte in data:
            byte_counts[byte] += 1
        
        # Calculate entropy
        entropy = 0
        data_len = len(data)
        for count in byte_counts:
            if count > 0:
                probability = count / data_len
                entropy -= probability * math.log2(probability)
        
        return entropy

    def _analyze_strings(self, file_path: str) -> Dict[str, Any]:
        """Analyze strings in the file"""
        strings_analysis = {
            "total_strings": 0,
            "string_categories": {
                "urls": [],
                "file_paths": [],
                "registry_keys": [],
                "ip_addresses": [],
                "domains": [],
                "error_messages": [],
                "debug_strings": []
            },
            "string_statistics": {}
        }
        
        try:
            with open(file_path, 'rb') as f:
                data = f.read()
            
            # Extract ASCII and Unicode strings
            ascii_strings = self._extract_strings(data, encoding='ascii')
            unicode_strings = self._extract_strings(data, encoding='unicode')
            
            all_strings = ascii_strings + unicode_strings
            strings_analysis["total_strings"] = len(all_strings)
            
            # Categorize strings
            for string in all_strings:
                self._categorize_string(string, strings_analysis["string_categories"])
            
            # Calculate statistics
            if all_strings:
                string_lengths = [len(s) for s in all_strings]
                strings_analysis["string_statistics"] = {
                    "average_length": sum(string_lengths) / len(string_lengths),
                    "max_length": max(string_lengths),
                    "min_length": min(string_lengths),
                    "ascii_count": len(ascii_strings),
                    "unicode_count": len(unicode_strings)
                }
        
        except Exception as e:
            logger.warning(f"Error analyzing strings: {e}")
        
        return strings_analysis

    def _extract_strings(self, data: bytes, encoding: str = 'ascii', min_length: int = 4) -> List[str]:
        """Extract strings from binary data"""
        strings = []
        current_string = ""
        
        if encoding == 'ascii':
            for byte in data:
                if 32 <= byte <= 126:  # Printable ASCII
                    current_string += chr(byte)
                else:
                    if len(current_string) >= min_length:
                        strings.append(current_string)
                    current_string = ""
        elif encoding == 'unicode':
            # Simple Unicode extraction (UTF-16 LE)
            for i in range(0, len(data) - 1, 2):
                try:
                    char = data[i:i+2].decode('utf-16le')
                    if char.isprintable() and not char.isspace():
                        current_string += char
                    else:
                        if len(current_string) >= min_length:
                            strings.append(current_string)
                        current_string = ""
                except:
                    if len(current_string) >= min_length:
                        strings.append(current_string)
                    current_string = ""
        
        # Don't forget the last string
        if len(current_string) >= min_length:
            strings.append(current_string)
        
        return strings[:1000]  # Limit to prevent memory issues

    def _categorize_string(self, string: str, categories: Dict[str, List[str]]):
        """Categorize a string into appropriate category"""
        string_lower = string.lower()
        
        # URLs
        if any(protocol in string_lower for protocol in ["http://", "https://", "ftp://"]):
            categories["urls"].append(string)
        # File paths
        elif any(path_indicator in string_lower for path_indicator in ["c:\\", "\\windows\\", "\\system32\\", ".exe", ".dll"]):
            categories["file_paths"].append(string)
        # Registry keys
        elif any(reg_indicator in string_lower for reg_indicator in ["hkey_", "software\\", "system\\", "currentversion"]):
            categories["registry_keys"].append(string)
        # IP addresses
        elif self._looks_like_ip(string):
            categories["ip_addresses"].append(string)
        # Domains
        elif self._looks_like_domain(string):
            categories["domains"].append(string)
        # Error messages
        elif any(error_word in string_lower for error_word in ["error", "failed", "exception", "cannot", "unable"]):
            categories["error_messages"].append(string)
        # Debug strings
        elif any(debug_word in string_lower for debug_word in ["debug", "trace", "log", "printf", "cout"]):
            categories["debug_strings"].append(string)

    def _looks_like_ip(self, string: str) -> bool:
        """Check if string looks like an IP address"""
        import re
        ip_pattern = re.compile(r'^(\d{1,3}\.){3}\d{1,3}$')
        if ip_pattern.match(string):
            try:
                parts = string.split('.')
                return all(0 <= int(part) <= 255 for part in parts)
            except ValueError:
                return False
        return False

    def _looks_like_domain(self, string: str) -> bool:
        """Check if string looks like a domain name"""
        import re
        domain_pattern = re.compile(r'^[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*\.[a-zA-Z]{2,}$')
        return bool(domain_pattern.match(string)) and len(string) >= 4

    def _extract_version_info(self, pe: pefile.PE) -> Dict[str, Any]:
        """Extract version information"""
        version_info = {}
        
        if hasattr(pe, 'VS_VERSIONINFO'):
            for version_entry in pe.VS_VERSIONINFO:
                if hasattr(version_entry, 'StringTable'):
                    for string_table in version_entry.StringTable:
                        for entry in string_table.entries.items():
                            key = entry[0].decode('utf-8', errors='ignore')
                            value = entry[1].decode('utf-8', errors='ignore')
                            version_info[key] = value
        
        return version_info

    def _extract_digital_signature(self, pe: pefile.PE) -> Dict[str, Any]:
        """Extract digital signature information"""
        signature_info = {
            "is_signed": False,
            "signature_present": False,
            "certificate_info": {}
        }
        
        if hasattr(pe, 'DIRECTORY_ENTRY_SECURITY'):
            signature_info["signature_present"] = True
            signature_info["is_signed"] = True
            # Additional certificate parsing would go here
        
        return signature_info

    def _extract_packer_indicators(self, pe: pefile.PE) -> Dict[str, Any]:
        """Extract indicators of packing/obfuscation"""
        packer_indicators = {
            "section_analysis": {},
            "entry_point_analysis": {},
            "import_analysis": {},
            "potential_packers": []
        }
        
        # Analyze sections for packing indicators
        sections = pe.sections
        if len(sections) < 3:
            packer_indicators["section_analysis"]["few_sections"] = True
        
        # Check section names for packer signatures
        packer_names = ["upx", "aspack", "pecompact", "petite", "fsg", "mew", "themida", "vmprotect"]
        for section in sections:
            section_name = section.Name.decode('utf-8', errors='ignore').strip('\x00').lower()
            for packer in packer_names:
                if packer in section_name:
                    packer_indicators["potential_packers"].append(packer.upper())
        
        # Check entry point section
        if hasattr(pe, 'OPTIONAL_HEADER'):
            entry_point = pe.OPTIONAL_HEADER.AddressOfEntryPoint
            entry_point_section = None
            
            for section in sections:
                if (section.VirtualAddress <= entry_point < 
                    section.VirtualAddress + section.Misc_VirtualSize):
                    entry_point_section = section.Name.decode('utf-8', errors='ignore').strip('\x00')
                    break
            
            packer_indicators["entry_point_analysis"]["section"] = entry_point_section
            packer_indicators["entry_point_analysis"]["unusual"] = entry_point_section not in [".text", "CODE"]
        
        # Analyze import table
        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
            import_count = sum(len(entry.imports) for entry in pe.DIRECTORY_ENTRY_IMPORT)
            packer_indicators["import_analysis"]["total_imports"] = import_count
            packer_indicators["import_analysis"]["few_imports"] = import_count < 5
        
        return packer_indicators

    def _extract_architecture_info(self, pe: pefile.PE) -> Dict[str, Any]:
        """Extract architecture and compilation information"""
        arch_info = {
            "architecture": self._get_machine_type_name(pe.FILE_HEADER.Machine),
            "is_32bit": pe.FILE_HEADER.Machine == 0x14c,
            "is_64bit": pe.FILE_HEADER.Machine == 0x8664,
            "is_dll": bool(pe.FILE_HEADER.Characteristics & 0x2000),
            "is_executable": bool(pe.FILE_HEADER.Characteristics & 0x0002)
        }
        
        return arch_info

    def _extract_compilation_info(self, pe: pefile.PE) -> Dict[str, Any]:
        """Extract compilation-related information"""
        compile_info = {
            "compile_timestamp": pe.FILE_HEADER.TimeDateStamp,
            "compile_date": datetime.fromtimestamp(pe.FILE_HEADER.TimeDateStamp).isoformat(),
            "linker_version": None,
            "rich_header_present": False
        }
        
        if hasattr(pe, 'OPTIONAL_HEADER'):
            compile_info["linker_version"] = f"{pe.OPTIONAL_HEADER.MajorLinkerVersion}.{pe.OPTIONAL_HEADER.MinorLinkerVersion}"
        
        # Check for Rich header (compilation environment info)
        if hasattr(pe, 'RICH_HEADER'):
            compile_info["rich_header_present"] = True
        
        return compile_info

    def _yara_scan(self, file_path: str) -> List[Dict[str, Any]]:
        """Perform YARA scanning (placeholder)"""
        yara_matches = []
        
        # This is a placeholder - in a real implementation, you would:
        # 1. Load YARA rules from a rules directory
        # 2. Compile the rules
        # 3. Scan the file
        # 4. Return matches
        
        try:
            # Example of what YARA scanning would look like:
            # rules = yara.compile(filepath='/path/to/rules.yar')
            # matches = rules.match(file_path)
            # for match in matches:
            #     yara_matches.append({
            #         "rule_name": match.rule,
            #         "tags": match.tags,
            #         "strings": [str(s) for s in match.strings]
            #     })
            pass
        except Exception as e:
            logger.warning(f"YARA scanning failed: {e}")
        
        return yara_matches

    def _extract_factual_iocs(self, pe_data: Dict[str, Any], file_path: str) -> List[IOC]:
        """Extract factual IOCs from PE analysis"""
        iocs = []
        
        # File hashes
        file_info = pe_data.get("file_info", {})
        for hash_type in ["md5", "sha1", "sha256"]:
            hash_value = file_info.get(hash_type)
            if hash_value:
                iocs.append(IOC(
                    type=IOCType.FILE_HASH,
                    value=hash_value,
                    confidence=1.0,
                    source="PE Analysis",
                    description=f"{hash_type.upper()} hash of analyzed file"
                ))
        
        # File path
        iocs.append(IOC(
            type=IOCType.FILE_PATH,
            value=file_path,
            confidence=1.0,
            source="PE Analysis",
            description="Path to analyzed PE file"
        ))
        
        # URLs found in strings
        strings_analysis = pe_data.get("strings_analysis", {})
        string_categories = strings_analysis.get("string_categories", {})
        
        for url in string_categories.get("urls", [])[:20]:  # Limit to prevent overflow
            iocs.append(IOC(
                type=IOCType.URL,
                value=url,
                confidence=1.0,
                source="PE Strings",
                description="URL found in PE file strings"
            ))
        
        # IP addresses found in strings
        for ip in string_categories.get("ip_addresses", [])[:20]:
            iocs.append(IOC(
                type=IOCType.IP_ADDRESS,
                value=ip,
                confidence=1.0,
                source="PE Strings",
                description="IP address found in PE file strings"
            ))
        
        # Registry keys found in strings
        for reg_key in string_categories.get("registry_keys", [])[:20]:
            iocs.append(IOC(
                type=IOCType.REGISTRY_KEY,
                value=reg_key,
                confidence=1.0,
                source="PE Strings",
                description="Registry key found in PE file strings"
            ))
        
        # File paths found in strings
        for file_path_str in string_categories.get("file_paths", [])[:20]:
            iocs.append(IOC(
                type=IOCType.FILE_PATH,
                value=file_path_str,
                confidence=1.0,
                source="PE Strings",
                description="File path found in PE file strings"
            ))
        
        return iocs

    def _create_error_result(self, error_message: str) -> AnalysisResult:
        """Create error result for failed analysis"""
        return AnalysisResult(
            file_path="",
            parser_name=self.name,
            analysis_type="PE File Data Extraction",
            timestamp=datetime.now(),
            summary=f"Data extraction failed: {error_message}",
            details={"error": error_message},
            threats_detected=[],
            iocs_found=[],
            severity=Severity.LOW,
            risk_score=0.0,
            recommendations=["Fix extraction error and retry analysis"]
        )

def create_parser():
    """Factory function to create parser instance"""
    return PEAnalyzer()